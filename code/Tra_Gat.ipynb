{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下載 AAPL 的資料...\n",
      "下載 MSFT 的資料...\n",
      "下載 GOOGL 的資料...\n",
      "下載 AMZN 的資料...\n",
      "下載 TSLA 的資料...\n",
      "下載 NVDA 的資料...\n",
      "下載 JPM 的資料...\n",
      "下載 V 的資料...\n",
      "下載 DIS 的資料...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下載 NFLX 的資料...\n",
      "Price       Adj Close      Close       High        Low       Open     Volume\n",
      "Ticker           AAPL       AAPL       AAPL       AAPL       AAPL       AAPL\n",
      "Date                                                                        \n",
      "2020-01-02  72.716064  75.087502  75.150002  73.797501  74.059998  135480400\n",
      "2020-01-03  72.009117  74.357498  75.144997  74.125000  74.287498  146322800\n",
      "2020-01-06  72.582901  74.949997  74.989998  73.187500  73.447502  118387200\n",
      "2020-01-07  72.241554  74.597504  75.224998  74.370003  74.959999  108872000\n",
      "2020-01-08  73.403648  75.797501  76.110001  74.290001  74.290001  132079200\n"
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA', 'NVDA', 'JPM', 'V', 'DIS', 'NFLX']\n",
    "start_date = '2020-01-01'\n",
    "end_date = '2024-01-01'\n",
    "\n",
    "data_dir = \"data\"\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "data_dict = {}\n",
    "for ticker in tickers:\n",
    "    file_path = os.path.join(data_dir, f\"{ticker}.csv\")\n",
    "    print(f\"下載 {ticker} 的資料...\")\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, auto_adjust=False)\n",
    "    data_dict[ticker] = df\n",
    "\n",
    "# 測試：印出某隻股票的前五筆資料\n",
    "print(data_dict['AAPL'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義技術指標計算函數\n",
    "def compute_technical_indicators(df):\n",
    "    df = df.copy()\n",
    "    # 移動平均線\n",
    "    df['MA10'] = df['Close'].rolling(window=10).mean()\n",
    "    df['MA50'] = df['Close'].rolling(window=50).mean()\n",
    "    \n",
    "    # RSI (14 天)\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.clip(lower=0)\n",
    "    loss = -delta.clip(upper=0)\n",
    "    avg_gain = gain.rolling(window=14).mean()\n",
    "    avg_loss = loss.rolling(window=14).mean()\n",
    "    rs = avg_gain / (avg_loss + 1e-6)\n",
    "    df['RSI'] = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Bollinger Bands (20 天)\n",
    "    df['MA20'] = df['Close'].rolling(window=20).mean()\n",
    "    df['STD20'] = df['Close'].rolling(window=20).std()\n",
    "    df['Bollinger_Upper'] = df['MA20'] + 2 * df['STD20']\n",
    "    df['Bollinger_Lower'] = df['MA20'] - 2 * df['STD20']\n",
    "    \n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "# 為每支股票計算技術指標\n",
    "for ticker in tickers:\n",
    "    data_dict[ticker] = compute_technical_indicators(data_dict[ticker])\n",
    "\n",
    "# 取所有股票的共同日期（避免因交易日不同而導致對齊問題）\n",
    "common_dates = set.intersection(*[set(df.index) for df in data_dict.values()])\n",
    "common_dates = sorted(list(common_dates))\n",
    "for ticker in tickers:\n",
    "    data_dict[ticker] = data_dict[ticker].loc[common_dates]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
      "/tmp/ipykernel_2151/1612388131.py:22: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n"
     ]
    }
   ],
   "source": [
    "# 設定使用之特徵\n",
    "features_list = ['Close', 'Volume', 'MA10', 'MA50', 'RSI', 'Bollinger_Upper', 'Bollinger_Lower']\n",
    "num_features = len(features_list)\n",
    "num_stocks = len(tickers)\n",
    "\n",
    "# 使用滑動視窗法構建資料樣本：以過去 30 日數據預測第 31 日收盤價\n",
    "window_size = 30\n",
    "num_samples = len(common_dates) - window_size\n",
    "\n",
    "# 初始化 X 與 y\n",
    "# X 的 shape 為 (num_samples, num_stocks, window_size, num_features)\n",
    "# y 的 shape 為 (num_samples, num_stocks)\n",
    "X = np.zeros((num_samples, num_stocks, window_size, num_features))\n",
    "y = np.zeros((num_samples, num_stocks))\n",
    "\n",
    "for stock_idx, ticker in enumerate(tickers):\n",
    "    df = data_dict[ticker]\n",
    "    data_array = df[features_list].values  # shape (T, num_features)\n",
    "    close_array = df['Close'].values         # shape (T,)\n",
    "    for i in range(num_samples):\n",
    "        X[i, stock_idx] = data_array[i:i+window_size]\n",
    "        y[i, stock_idx] = close_array[i+window_size]  # 預測下一日收盤價\n",
    "\n",
    "# 標準化特徵（所有樣本共用一組 scaler）\n",
    "scaler = StandardScaler()\n",
    "X_reshaped = X.reshape(-1, num_features)\n",
    "X_scaled = scaler.fit_transform(X_reshaped)\n",
    "X = X_scaled.reshape(num_samples, num_stocks, window_size, num_features)\n",
    "\n",
    "# 切分訓練/測試集（80% 訓練，20% 測試）\n",
    "split_idx = int(num_samples * 0.8)\n",
    "X_train = X[:split_idx]\n",
    "y_train = y[:split_idx]\n",
    "X_test = X[split_idx:]\n",
    "y_test = y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        # X: (num_samples, num_stocks, window_size, features)\n",
    "        # y: (num_samples, num_stocks)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = StockDataset(X_train, y_train)\n",
    "test_dataset = StockDataset(X_test, y_test)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用訓練集期間的收盤價來計算各股票間皮爾遜相關係數\n",
    "train_dates = common_dates[window_size:split_idx+window_size]  # 標籤開始的日期\n",
    "prices_train = []\n",
    "for ticker in tickers:\n",
    "    df = data_dict[ticker]\n",
    "    prices = df['Close'].loc[train_dates].values\n",
    "    prices_train.append(prices)\n",
    "prices_train = np.array(prices_train)  # shape: (num_stocks, num_train_days)\n",
    "prices_train = np.squeeze(prices_train, axis=-1) #不知道為啥多一維\n",
    "\n",
    "corr_matrix = np.corrcoef(prices_train)\n",
    "threshold = 0.5\n",
    "# 構造鄰接矩陣：若相關係數絕對值大於門檻則視為連接\n",
    "adj_matrix = (np.abs(corr_matrix) > threshold).astype(np.float32)\n",
    "# 加入自迴路\n",
    "np.fill_diagonal(adj_matrix, 1.0)\n",
    "adj_tensor = torch.tensor(adj_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1 Transformer 模組（處理時間序列）\n",
    "class TimeSeriesTransformer(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TimeSeriesTransformer, self).__init__()\n",
    "        self.input_linear = nn.Linear(feature_size, d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, feature_size)\n",
    "        x = self.input_linear(x)         # (batch, seq_len, d_model)\n",
    "        x = x.transpose(0, 1)            # (seq_len, batch, d_model)\n",
    "        x = self.transformer_encoder(x)  # (seq_len, batch, d_model)\n",
    "        x = x.mean(dim=0)                # (batch, d_model) 使用均值池化\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4.2 GAT 模組（學習股票間關聯）\n",
    "class GraphAttentionLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features, dropout=0.1, alpha=0.2):\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.W = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.a = nn.Linear(2*out_features, 1, bias=False)\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, h, adj):\n",
    "        # h: (N, in_features)，其中 N 為股票數\n",
    "        Wh = self.W(h)  # (N, out_features)\n",
    "        N = Wh.size(0)\n",
    "        # 計算所有節點對的 attention 分數\n",
    "        # 先組合所有 (Wh_i || Wh_j)\n",
    "        a_input = torch.cat([Wh.repeat(1, N).view(N*N, -1), Wh.repeat(N, 1)], dim=1)\n",
    "        a_input = a_input.view(N, N, -1)  # (N, N, 2*out_features)\n",
    "        e = self.leakyrelu(self.a(a_input)).squeeze(2)  # (N, N)\n",
    "        \n",
    "        # 掩蔽非鄰居節點：將無連接處的分數設為 -inf\n",
    "        zero_vec = -9e15 * torch.ones_like(e)\n",
    "        attention = torch.where(adj > 0, e, zero_vec)\n",
    "        attention = F.softmax(attention, dim=1)\n",
    "        attention = self.dropout(attention)\n",
    "        h_prime = torch.matmul(attention, Wh)\n",
    "        return h_prime, attention\n",
    "\n",
    "# 4.3 融合層：將 Transformer 與 GAT 的輸出結合\n",
    "class FusionMLP(nn.Module):\n",
    "    def __init__(self, in_features, hidden_size, out_features):\n",
    "        super(FusionMLP, self).__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, out_features)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.mlp(x)\n",
    "    \n",
    "# 4.4 完整 TGNN 模型：對每支股票先用 Transformer 得到時間特徵，再利用 GAT 學習股票間關聯，最後融合預測未來收盤價\n",
    "class TGNN(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers, gat_hidden, fusion_hidden):\n",
    "        super(TGNN, self).__init__()\n",
    "        self.transformer = TimeSeriesTransformer(feature_size, d_model, nhead, num_layers)\n",
    "        self.gat = GraphAttentionLayer(d_model, gat_hidden)\n",
    "        # 融合層輸入為 [Transformer 輸出 (d_model) || GAT 輸出 (gat_hidden)]\n",
    "        self.fusion = FusionMLP(d_model + gat_hidden, fusion_hidden, 1)  # 每支股票預測一個數值\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        # x: (num_stocks, window_size, feature_size)\n",
    "        num_stocks = x.size(0)\n",
    "        transformer_outputs = []\n",
    "        # 分別處理每支股票的時間序列\n",
    "        for i in range(num_stocks):\n",
    "            xi = x[i].unsqueeze(0)  # (1, window_size, feature_size)\n",
    "            hi = self.transformer(xi)  # (1, d_model)\n",
    "            transformer_outputs.append(hi)\n",
    "        transformer_outputs = torch.cat(transformer_outputs, dim=0)  # (num_stocks, d_model)\n",
    "        \n",
    "        # 利用 GAT 模組捕捉股票間關聯\n",
    "        gat_output, _ = self.gat(transformer_outputs, adj)\n",
    "        # 融合兩者資訊\n",
    "        fusion_input = torch.cat([transformer_outputs, gat_output], dim=1)  # (num_stocks, d_model + gat_hidden)\n",
    "        prediction = self.fusion(fusion_input)  # (num_stocks, 1)\n",
    "        prediction = prediction.squeeze(1)       # (num_stocks)\n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerOnlyModel(nn.Module):\n",
    "    def __init__(self, feature_size, d_model, nhead, num_layers, dropout=0.1):\n",
    "        super(TransformerOnlyModel, self).__init__()\n",
    "        self.transformer = TimeSeriesTransformer(feature_size, d_model, nhead, num_layers, dropout)\n",
    "        self.out = nn.Linear(d_model, 1)  # 每檔股票最後輸出一個數值（未來股價或報酬等）\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (num_stocks, window_size, feature_size)\n",
    "        這裡的作法是逐檔股票分別跑一次 Transformer，再將結果合併。\n",
    "        \"\"\"\n",
    "        num_stocks = x.size(0)\n",
    "        outputs = []\n",
    "        for i in range(num_stocks):\n",
    "            xi = x[i].unsqueeze(0)    # (1, window_size, feature_size)\n",
    "            hi = self.transformer(xi) # (1, d_model)\n",
    "            out_i = self.out(hi)      # (1, 1)\n",
    "            outputs.append(out_i)\n",
    "        # 將每檔股票的預測結果合併成 (num_stocks,) 向量\n",
    "        predictions = torch.cat(outputs, dim=0).squeeze(1)  # (num_stocks)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練(只有 Transformer)...\n",
      "Epoch 1/50, Train Loss: 44470.2751\n",
      "Epoch 1/50, Test Loss: 46959.8863\n",
      "Epoch 2/50, Train Loss: 43748.1565\n",
      "Epoch 2/50, Test Loss: 46250.9624\n",
      "Epoch 3/50, Train Loss: 43024.2286\n",
      "Epoch 3/50, Test Loss: 45423.2193\n",
      "Epoch 4/50, Train Loss: 42203.8555\n",
      "Epoch 4/50, Test Loss: 44488.0259\n",
      "Epoch 5/50, Train Loss: 41287.0360\n",
      "Epoch 5/50, Test Loss: 43457.9168\n",
      "Epoch 6/50, Train Loss: 40289.4800\n",
      "Epoch 6/50, Test Loss: 42336.5753\n",
      "Epoch 7/50, Train Loss: 39219.6423\n",
      "Epoch 7/50, Test Loss: 41133.7917\n",
      "Epoch 8/50, Train Loss: 38084.6521\n",
      "Epoch 8/50, Test Loss: 39865.9916\n",
      "Epoch 9/50, Train Loss: 36895.4139\n",
      "Epoch 9/50, Test Loss: 38546.1020\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m x_sample \u001b[38;5;241m=\u001b[39m batch_X[i]  \u001b[38;5;66;03m# (num_stocks, window_size, feature_size)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m y_sample \u001b[38;5;241m=\u001b[39m batch_y[i]  \u001b[38;5;66;03m# (num_stocks)\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_sample\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (num_stocks)\u001b[39;00m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y_sample)\n\u001b[1;32m     39\u001b[0m batch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[15], line 16\u001b[0m, in \u001b[0;36mTransformerOnlyModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_stocks):\n\u001b[1;32m     15\u001b[0m     xi \u001b[38;5;241m=\u001b[39m x[i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)    \u001b[38;5;66;03m# (1, window_size, feature_size)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     hi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxi\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (1, d_model)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     out_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(hi)      \u001b[38;5;66;03m# (1, 1)\u001b[39;00m\n\u001b[1;32m     18\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(out_i)\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mTimeSeriesTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_linear(x)         \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     12\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)            \u001b[38;5;66;03m# (seq_len, batch, d_model)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransformer_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (seq_len, batch, d_model)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)                \u001b[38;5;66;03m# (batch, d_model) 使用均值池化\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:416\u001b[0m, in \u001b[0;36mTransformerEncoder.forward\u001b[0;34m(self, src, mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    413\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m _detect_is_causal_mask(mask, is_causal, seq_len)\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 416\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msrc_key_padding_mask_for_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_nested:\n\u001b[1;32m    419\u001b[0m     output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mto_padded_tensor(\u001b[38;5;241m0.\u001b[39m, src\u001b[38;5;241m.\u001b[39msize())\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:749\u001b[0m, in \u001b[0;36mTransformerEncoderLayer.forward\u001b[0;34m(self, src, src_mask, src_key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    747\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 749\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sa_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msrc_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    750\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ff_block(x))\n\u001b[1;32m    752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:757\u001b[0m, in \u001b[0;36mTransformerEncoderLayer._sa_block\u001b[0;34m(self, x, attn_mask, key_padding_mask, is_causal)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_sa_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor,\n\u001b[1;32m    756\u001b[0m               attn_mask: Optional[Tensor], key_padding_mask: Optional[Tensor], is_causal: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 757\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    758\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    759\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    760\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/functional.py:5564\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5563\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n\u001b[0;32m-> 5564\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbsz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_output\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5565\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_batched:\n\u001b[1;32m   5566\u001b[0m     \u001b[38;5;66;03m# squeeze the output if input was unbatched\u001b[39;00m\n\u001b[1;32m   5567\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 超參數設定\n",
    "feature_size = 7   # 例如你有 Close, Volume, MA10, MA50, RSI, Bollinger_Upper, Bollinger_Lower\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerOnlyModel(feature_size, d_model, nhead, num_layers, dropout).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 50\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"開始訓練(只有 Transformer)...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device).float()  # shape: (batch, num_stocks, window_size, feature_size)\n",
    "        batch_y = batch_y.to(device).float()  # shape: (batch, num_stocks)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 逐筆樣本處理\n",
    "        batch_loss = 0.0\n",
    "        for i in range(batch_X.size(0)):\n",
    "            x_sample = batch_X[i]  # (num_stocks, window_size, feature_size)\n",
    "            y_sample = batch_y[i]  # (num_stocks)\n",
    "            pred = model(x_sample) # (num_stocks)\n",
    "            loss = criterion(pred, y_sample)\n",
    "            batch_loss += loss\n",
    "        \n",
    "        batch_loss = batch_loss / batch_X.size(0)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += batch_loss.item() * batch_X.size(0)\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # 計算測試集損失\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "            batch_loss = 0.0\n",
    "            for i in range(batch_X.size(0)):\n",
    "                x_sample = batch_X[i]\n",
    "                y_sample = batch_y[i]\n",
    "                pred = model(x_sample)\n",
    "                loss = criterion(pred, y_sample)\n",
    "                batch_loss += loss\n",
    "            batch_loss = batch_loss / batch_X.size(0)\n",
    "            test_loss += batch_loss.item() * batch_X.size(0)\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# 繪製訓練損失和測試損失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train and Test Loss (Only Transformer)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        for i in range(batch_X.size(0)):\n",
    "            x_sample = batch_X[i]\n",
    "            y_sample = batch_y[i]\n",
    "            pred = model(x_sample)\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(y_sample.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(all_labels.flatten(), all_preds.flatten()))\n",
    "mae  = mean_absolute_error(all_labels.flatten(), all_preds.flatten())\n",
    "r2   = r2_score(all_labels.flatten(), all_preds.flatten())\n",
    "\n",
    "print(f\"\\n(Only Transformer) Test RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hybird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenny1208/anaconda3/envs/Torch/lib/python3.10/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始訓練...\n",
      "Epoch 1/50, Train Loss: 44587.9990\n",
      "Epoch 1/50, Test Loss: 45232.2757\n",
      "Epoch 2/50, Train Loss: 38282.5768\n",
      "Epoch 2/50, Test Loss: 32988.5565\n",
      "Epoch 3/50, Train Loss: 23317.6559\n",
      "Epoch 3/50, Test Loss: 14817.0525\n",
      "Epoch 4/50, Train Loss: 11657.8856\n",
      "Epoch 4/50, Test Loss: 5777.9952\n",
      "Epoch 5/50, Train Loss: 8019.2314\n",
      "Epoch 5/50, Test Loss: 5338.6648\n",
      "Epoch 6/50, Train Loss: 7157.2789\n",
      "Epoch 6/50, Test Loss: 3402.1003\n",
      "Epoch 7/50, Train Loss: 5095.0245\n",
      "Epoch 7/50, Test Loss: 2649.8622\n",
      "Epoch 8/50, Train Loss: 4214.4008\n",
      "Epoch 8/50, Test Loss: 2474.3503\n",
      "Epoch 9/50, Train Loss: 3522.5104\n",
      "Epoch 9/50, Test Loss: 2850.5775\n",
      "Epoch 10/50, Train Loss: 3359.2849\n",
      "Epoch 10/50, Test Loss: 2040.3230\n",
      "Epoch 11/50, Train Loss: 2740.5791\n",
      "Epoch 11/50, Test Loss: 1974.6245\n",
      "Epoch 12/50, Train Loss: 2244.7951\n",
      "Epoch 12/50, Test Loss: 1646.3481\n",
      "Epoch 13/50, Train Loss: 2240.1051\n",
      "Epoch 13/50, Test Loss: 1351.8825\n",
      "Epoch 14/50, Train Loss: 1775.7283\n",
      "Epoch 14/50, Test Loss: 1090.8639\n",
      "Epoch 15/50, Train Loss: 1660.9697\n",
      "Epoch 15/50, Test Loss: 1121.9512\n",
      "Epoch 16/50, Train Loss: 1474.9267\n",
      "Epoch 16/50, Test Loss: 941.2321\n",
      "Epoch 17/50, Train Loss: 1223.1889\n",
      "Epoch 17/50, Test Loss: 792.2793\n",
      "Epoch 18/50, Train Loss: 1135.0477\n",
      "Epoch 18/50, Test Loss: 787.9356\n",
      "Epoch 19/50, Train Loss: 1004.2726\n",
      "Epoch 19/50, Test Loss: 479.0642\n",
      "Epoch 20/50, Train Loss: 869.9065\n",
      "Epoch 20/50, Test Loss: 566.3957\n",
      "Epoch 21/50, Train Loss: 885.3937\n",
      "Epoch 21/50, Test Loss: 574.0308\n",
      "Epoch 22/50, Train Loss: 732.1181\n",
      "Epoch 22/50, Test Loss: 380.9956\n",
      "Epoch 23/50, Train Loss: 648.7248\n",
      "Epoch 23/50, Test Loss: 454.0390\n",
      "Epoch 24/50, Train Loss: 529.3624\n",
      "Epoch 24/50, Test Loss: 443.9971\n",
      "Epoch 25/50, Train Loss: 465.5706\n",
      "Epoch 25/50, Test Loss: 298.2542\n",
      "Epoch 26/50, Train Loss: 352.1815\n",
      "Epoch 26/50, Test Loss: 285.4935\n",
      "Epoch 27/50, Train Loss: 347.1927\n",
      "Epoch 27/50, Test Loss: 379.1401\n",
      "Epoch 28/50, Train Loss: 300.2817\n",
      "Epoch 28/50, Test Loss: 382.8710\n",
      "Epoch 29/50, Train Loss: 252.0348\n",
      "Epoch 29/50, Test Loss: 315.4656\n",
      "Epoch 30/50, Train Loss: 263.6121\n",
      "Epoch 30/50, Test Loss: 316.5539\n",
      "Epoch 31/50, Train Loss: 272.8568\n",
      "Epoch 31/50, Test Loss: 733.7211\n",
      "Epoch 32/50, Train Loss: 286.9510\n",
      "Epoch 32/50, Test Loss: 285.7088\n",
      "Epoch 33/50, Train Loss: 245.1611\n",
      "Epoch 33/50, Test Loss: 250.9135\n",
      "Epoch 34/50, Train Loss: 208.7590\n",
      "Epoch 34/50, Test Loss: 267.3873\n",
      "Epoch 35/50, Train Loss: 199.9311\n",
      "Epoch 35/50, Test Loss: 242.6706\n",
      "Epoch 36/50, Train Loss: 206.7016\n",
      "Epoch 36/50, Test Loss: 317.3749\n",
      "Epoch 37/50, Train Loss: 225.8413\n",
      "Epoch 37/50, Test Loss: 310.1028\n",
      "Epoch 38/50, Train Loss: 162.0069\n",
      "Epoch 38/50, Test Loss: 344.5499\n",
      "Epoch 39/50, Train Loss: 165.7317\n",
      "Epoch 39/50, Test Loss: 265.8706\n",
      "Epoch 40/50, Train Loss: 162.7068\n",
      "Epoch 40/50, Test Loss: 262.5498\n",
      "Epoch 41/50, Train Loss: 159.7594\n",
      "Epoch 41/50, Test Loss: 299.4200\n",
      "Epoch 42/50, Train Loss: 169.5866\n",
      "Epoch 42/50, Test Loss: 239.2722\n",
      "Epoch 43/50, Train Loss: 147.8071\n",
      "Epoch 43/50, Test Loss: 268.9868\n",
      "Epoch 44/50, Train Loss: 151.2588\n",
      "Epoch 44/50, Test Loss: 303.0145\n",
      "Epoch 45/50, Train Loss: 157.7297\n",
      "Epoch 45/50, Test Loss: 538.7139\n",
      "Epoch 46/50, Train Loss: 153.4835\n",
      "Epoch 46/50, Test Loss: 271.8532\n",
      "Epoch 47/50, Train Loss: 128.7911\n",
      "Epoch 47/50, Test Loss: 217.0377\n",
      "Epoch 48/50, Train Loss: 129.1376\n",
      "Epoch 48/50, Test Loss: 352.0772\n",
      "Epoch 49/50, Train Loss: 125.8650\n",
      "Epoch 49/50, Test Loss: 202.6953\n",
      "Epoch 50/50, Train Loss: 124.9355\n",
      "Epoch 50/50, Test Loss: 230.8336\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 模型超參數設定\n",
    "feature_size = num_features\n",
    "d_model = 64\n",
    "nhead = 4\n",
    "num_layers = 2\n",
    "gat_hidden = 32\n",
    "fusion_hidden = 64\n",
    "\n",
    "# 實例化模型\n",
    "model = TGNN(feature_size, d_model, nhead, num_layers, gat_hidden, fusion_hidden).to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "num_epochs = 50\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "print(\"開始訓練...\")\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        batch_X = batch_X.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = 0.0\n",
    "        for i in range(batch_X.size(0)):\n",
    "            x_sample = batch_X[i]\n",
    "            y_sample = batch_y[i]\n",
    "            pred = model(x_sample, adj_tensor.to(device))\n",
    "            loss = criterion(pred, y_sample)\n",
    "            batch_loss += loss\n",
    "        batch_loss = batch_loss / batch_X.size(0)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += batch_loss.item() * batch_X.size(0)\n",
    "    train_loss /= len(train_dataset)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}\")\n",
    "\n",
    "    # 計算測試集損失\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in test_loader:\n",
    "            batch_X = batch_X.to(device).float()\n",
    "            batch_y = batch_y.to(device).float()\n",
    "            batch_loss = 0.0\n",
    "            for i in range(batch_X.size(0)):\n",
    "                x_sample = batch_X[i]\n",
    "                y_sample = batch_y[i]\n",
    "                pred = model(x_sample, adj_tensor.to(device))\n",
    "                loss = criterion(pred, y_sample)\n",
    "                batch_loss += loss\n",
    "            batch_loss = batch_loss / batch_X.size(0)\n",
    "            test_loss += batch_loss.item() * batch_X.size(0)\n",
    "    test_loss /= len(test_dataset)\n",
    "    test_losses.append(test_loss)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Test Loss: {test_loss:.4f}\")\n",
    "    model.train()\n",
    "\n",
    "# 繪製訓練損失和測試損失\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Train and Test Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to tgnn_model.pth\n"
     ]
    }
   ],
   "source": [
    "model_save_path = \"tgnn_model.pth\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test RMSE: 15.1932, MAE: 9.9602, R2: 0.9807\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_y in test_loader:\n",
    "        batch_X = batch_X.to(device).float()\n",
    "        batch_y = batch_y.to(device).float()\n",
    "        for i in range(batch_X.size(0)):\n",
    "            x_sample = batch_X[i]\n",
    "            y_sample = batch_y[i]\n",
    "            pred = model(x_sample, adj_tensor.to(device))\n",
    "            all_preds.append(pred.cpu().numpy())\n",
    "            all_labels.append(y_sample.cpu().numpy())\n",
    "\n",
    "all_preds = np.array(all_preds)    # (num_test_samples, num_stocks)\n",
    "all_labels = np.array(all_labels)  # (num_test_samples, num_stocks)\n",
    "\n",
    "# 評估指標計算\n",
    "rmse = np.sqrt(mean_squared_error(all_labels.flatten(), all_preds.flatten()))\n",
    "mae  = mean_absolute_error(all_labels.flatten(), all_preds.flatten())\n",
    "r2   = r2_score(all_labels.flatten(), all_preds.flatten())\n",
    "print(f\"\\nTest RMSE: {rmse:.4f}, MAE: {mae:.4f}, R2: {r2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
